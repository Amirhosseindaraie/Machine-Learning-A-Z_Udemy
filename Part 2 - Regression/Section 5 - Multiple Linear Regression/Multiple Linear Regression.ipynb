{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index \n",
    "- [Assumptions of Linear Regression](#assumptions)\n",
    "- [Equation and Method](#equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing some basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assumptions'></a>\n",
    "### Assumptions of Linear Regression\n",
    "- Linearity\n",
    "- [Homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity)\n",
    "- [Multivariate normality](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)\n",
    "- Independence of errors\n",
    "- [Lack of multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)\n",
    "\n",
    "##### Dummy variable trap.\n",
    "The categorical variables should be split into proper dummy variables, and we should omit one of the columns of the dummy variables. By default our regression model will accound for the data without this last column and when it gets the other values, i.e a 1 in its corresponding column, it will factor in accordingly. The reason why we omit one of the columns is because of a phenomenon called dummy variable trap. The main culprit is multicolliniearity. The reason why this is dangeros to our model is that, all the variables should be linearly dependent, but in dummy variables if we add all the columns we will get 1, i.e they are linearly dependent. So, if we remove one column, we can eliminate dummy variable trap.\n",
    "\n",
    "##### P value\n",
    "The p-value is actually the probability of getting a sample like ours, or more extreme than ours IF the null hypothesis is true. So, we assume the null hypothesis is true and then determine how “strange” our sample really is. If it is not that strange (a large p-value) then we don’t change our mind about the null hypothesis. As the p-value gets smaller, we start wondering if the null really is true and well maybe we should change our minds (and reject the null hypothesis).\n",
    "\n",
    "- [Explanation 1](http://www.mathbootcamps.com/what-is-a-p-value/)\n",
    "- [Explanation 2](http://www.wikihow.com/Calculate-P-Value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='equation'></a>\n",
    "### Equation and Method\n",
    "\n",
    "Like simple linear regression, Multiple linear regression uses a linear equation with multiple independent variables to determine a dependent variable.\n",
    "\n",
    "$y$ = $b_{0}$ + $b_{1}$*$x_{1}$+ $b_{2}$*$x_{2}$+ $b_{3}$*$x_{3}$ ... + $b_{n}$*$x_{n}$\n",
    "\n",
    "\n",
    "#### Different methods\n",
    "- ##### All-in\n",
    "\n",
    "> Here we throw in all the variables that we have. We ususally do this when we have prior knowledge about our variables that they are significant or when a particular framework tells us that these variables should be included.\n",
    "\n",
    "- ##### Backward Elimination\n",
    "\n",
    "> 1) We first select a significant level to stay in the model(eg. sl=0.5).\n",
    "\n",
    "> 2) We fit the model with all the possible predictors(variables).\n",
    "\n",
    "> 3) Consider the prdictor with the highest P-value, if it is higher than sl, then remove that else end procedure.\n",
    "\n",
    "> 4) Fit the model without the removed predictor and go to previous step and do the same check.\n",
    "\n",
    "- ##### Forward Selection\n",
    "\n",
    "> 1) We first select a significant level to stay in the model(eg. sl=0.5).\n",
    "\n",
    "> 2) We fit all simple regression models and select one with the lowest P-value.\n",
    "\n",
    "> 3) We keep this variable and fit all possible models with one extra predictor added to the one.\n",
    "\n",
    "> 4) We then consider the predictor with the lowest P-value and if P < Sl go to previous step else end procedure.\n",
    "\n",
    "- ##### Bidirectional Elimination\n",
    "\n",
    "> 1) Select a significant value for entering and staying in the model.\n",
    "\n",
    "> 2) Perform forward selection with P < S-enter to enter.\n",
    "\n",
    "> 3) Perform all steps of backward elimination with old variables having P < S-stay to stay and go to previous step.\n",
    "\n",
    "> 4) No new variables can enter and no new can exit and then end the procedure.\n",
    "\n",
    "- ##### All possible models/ score comparison\n",
    "\n",
    "> Construct the model in all possible permutations and combinations of variables and compare their scores and select the best model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
