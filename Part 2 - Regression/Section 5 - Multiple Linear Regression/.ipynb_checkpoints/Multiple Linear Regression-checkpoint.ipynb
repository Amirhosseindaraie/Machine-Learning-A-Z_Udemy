{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index \n",
    "- [Assumptions of Linear Regression](#assumptions)\n",
    "- [Equation and Method](#equation)\n",
    "- [Excercise](#excercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing some basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assumptions'></a>\n",
    "### Assumptions of Linear Regression\n",
    "- Linearity\n",
    "- [Homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity)\n",
    "- [Multivariate normality](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)\n",
    "- Independence of errors\n",
    "- [Lack of multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)\n",
    "\n",
    "##### Dummy variable trap.\n",
    "The categorical variables should be split into proper dummy variables, and we should omit one of the columns of the dummy variables. By default our regression model will accound for the data without this last column and when it gets the other values, i.e a 1 in its corresponding column, it will factor in accordingly. The reason why we omit one of the columns is because of a phenomenon called dummy variable trap. The main culprit is multicolliniearity. The reason why this is dangeros to our model is that, all the variables should be linearly dependent, but in dummy variables if we add all the columns we will get 1, i.e they are linearly dependent. So, if we remove one column, we can eliminate dummy variable trap.\n",
    "\n",
    "##### P value\n",
    "The p-value is actually the probability of getting a sample like ours, or more extreme than ours IF the null hypothesis is true. So, we assume the null hypothesis is true and then determine how “strange” our sample really is. If it is not that strange (a large p-value) then we don’t change our mind about the null hypothesis. As the p-value gets smaller, we start wondering if the null really is true and well maybe we should change our minds (and reject the null hypothesis).\n",
    "\n",
    "- [Explanation 1](http://www.mathbootcamps.com/what-is-a-p-value/)\n",
    "- [Explanation 2](http://www.wikihow.com/Calculate-P-Value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='equation'></a>\n",
    "### Equation and Method\n",
    "\n",
    "Like simple linear regression, Multiple linear regression uses a linear equation with multiple independent variables to determine a dependent variable.\n",
    "\n",
    "$y$ = $b_{0}$ + $b_{1}$*$x_{1}$+ $b_{2}$*$x_{2}$+ $b_{3}$*$x_{3}$ ... + $b_{n}$*$x_{n}$\n",
    "\n",
    "\n",
    "#### Different methods\n",
    "- ##### All-in\n",
    "\n",
    "> Here we throw in all the variables that we have. We ususally do this when we have prior knowledge about our variables that they are significant or when a particular framework tells us that these variables should be included.\n",
    "\n",
    "- ##### Backward Elimination\n",
    "\n",
    "> 1) We first select a significant level to stay in the model(eg. sl=0.5).\n",
    "\n",
    "> 2) We fit the model with all the possible predictors(variables).\n",
    "\n",
    "> 3) Consider the prdictor with the highest P-value, if it is higher than sl, then remove that else end procedure.\n",
    "\n",
    "> 4) Fit the model without the removed predictor and go to previous step and do the same check.\n",
    "\n",
    "- ##### Forward Selection\n",
    "\n",
    "> 1) We first select a significant level to stay in the model(eg. sl=0.5).\n",
    "\n",
    "> 2) We fit all simple regression models and select one with the lowest P-value.\n",
    "\n",
    "> 3) We keep this variable and fit all possible models with one extra predictor added to the one.\n",
    "\n",
    "> 4) We then consider the predictor with the lowest P-value and if P < Sl go to previous step else end procedure.\n",
    "\n",
    "- ##### Bidirectional Elimination\n",
    "\n",
    "> 1) Select a significant value for entering and staying in the model.\n",
    "\n",
    "> 2) Perform forward selection with P < S-enter to enter.\n",
    "\n",
    "> 3) Perform all steps of backward elimination with old variables having P < S-stay to stay and go to previous step.\n",
    "\n",
    "> 4) No new variables can enter and no new can exit and then end the procedure.\n",
    "\n",
    "- ##### All possible models/ score comparison\n",
    "\n",
    "> Construct the model in all possible permutations and combinations of variables and compare their scores and select the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='excercise'></a>\n",
    "### Excersice\n",
    "\n",
    "The objective of this excerise is to inspect the data set of startups and build a model that can  predict the profit from the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.cross_validation import train_test_split;\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  dataset.iloc[:, :4].values\n",
    "y = dataset.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_x = LabelEncoder()\n",
    "x[:, 3] = label_x.fit_transform(x[:, 3])\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(categorical_features=[3])\n",
    "x = one_hot_encoder.fit_transform(x).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminating the dummy trap variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the model\n",
    "y_predict = regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backward elimination\n",
    "Eventhough we were able to build our model and predict the test values with some amount of accuracy, we still haven't looked into the factors by which the independent variables contribute towards the dependent variable that we are predicting. And also an important factor that we missed out is that, we did not account for the $B_{0}$ in our equation. When a multiple linear regression model is built, the coefficients are calculated with respect to the available columns in our dataset. Therefore it makes sense now as to why the $B_{0}$ was not calculated. To incorporate that, we simply need to add another column in our dataset, that is with full 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a row full of 1's for the intercept.\n",
    "x = np.append(arr = np.ones((50, 1)).astype(int), values = x, axis=1) \n",
    "# we switch inorder to have the first row of intercepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
