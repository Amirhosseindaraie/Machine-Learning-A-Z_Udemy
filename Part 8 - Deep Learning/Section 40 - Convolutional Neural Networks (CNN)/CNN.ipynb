{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index \n",
    "- [Equation and Method](#equation)\n",
    "- [Pre processing](#preprocessing)\n",
    "- [Building the model](#building)\n",
    "- [Result](#result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='equation'></a>\n",
    "### Equation and Method\n",
    "\n",
    "#### Architechture\n",
    "##### 1. Convolution\n",
    "\n",
    "This is the first step inside a convolutional neural network. The basic idea is that there will be a feature map/kernel/filter matrix and this is convoluted over the other pixels to get another resulting image. The basic idea behind convolution is to apply filters like blur, sharpen, edge detect, etc on images but they can also be made in the form of a feature, i.e of eyes or nose and they will be convoluted. Now for a particular classifier there will be lots of feature detectors and these feature detectors are responsible for detecting a particular feature inside the image.\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2017/11/convolution-example-matrix.gif' style='margin:0px' width=600>\n",
    "\n",
    "After training, the neural network will adjust the different parameters of the filter and then enhance and define in more detail the feature filters that the CNN is applying.\n",
    "\n",
    "##### 2. ReLu Layer\n",
    "\n",
    "The basic idea behind a ReLu Layer is to apply the function $max(x, 0)$. What happens is that it maps the negative values to 0 while leaving the positive values unchanged. The reason why we are doing this is to remove the nonlinearity. This somehow allows the CNN to detect better features and then detect the image. Another variant of this is Leaky ReLu. \n",
    "\n",
    "<img src='https://ml4a.github.io/images/figures/relu.png' style='margin:0px'>\n",
    "\n",
    "##### 3. Max Pooling\n",
    "\n",
    "The basic idea behind this layer is to produce spacial invariance. I.e we do not want our images to look like in a particular position all the time. This is fairly impossible for real world applications. So, we need to devise a system where the convoluted image is spacially invariant, i.e it can locate features located in other places too. So we use Max pooling.\n",
    "\n",
    "<img src='http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-05-at-2.18.38-PM.png' style='margin:0px' width=600>\n",
    "\n",
    "Another advantage of using this is that we can reduce the dimensions without affecting the precision and this is very good in terms of processing speed as higher dimensions take longer time to train. Another important aspect of doing this is that we can prevent overfitting as we are removing some of the information.\n",
    "\n",
    "##### 4. Flattening\n",
    "\n",
    "Flattening is the process of changing the $2*2$ feature map of the convoluted image and then changing that into a linear vector for input into an ANN.\n",
    "\n",
    "##### 5. Fully connected layer\n",
    "\n",
    "This is basically an ANN that selects the flattened layer as an input and functions as a regular ANN and uses back propogation to adjust the weights as well as the filters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='preprocessing'></a>\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available at (Super Data Science)[http://www.superdatascience.com/wp-content/uploads/2017/03/Convolutional-Neural-Networks.zip]. This is a sub dataset of (dogs vs cats)[https://www.kaggle.com/c/dogs-vs-cats]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='building'></a>\n",
    "### Building the model.\n",
    "Building a CNN using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Convolution2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Convolution2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.6563 - acc: 0.6121 - val_loss: 0.8265 - val_acc: 0.5714\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.5957 - acc: 0.6832 - val_loss: 0.5753 - val_acc: 0.7067\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 0.5533 - acc: 0.7191 - val_loss: 0.5208 - val_acc: 0.7402\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.5212 - acc: 0.7394 - val_loss: 0.5010 - val_acc: 0.7589\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.4958 - acc: 0.7565 - val_loss: 0.4827 - val_acc: 0.7746\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.4733 - acc: 0.7666 - val_loss: 0.4683 - val_acc: 0.7894\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.4588 - acc: 0.7839 - val_loss: 0.5158 - val_acc: 0.7643\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.4448 - acc: 0.7870 - val_loss: 0.5119 - val_acc: 0.7564\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.4305 - acc: 0.7991 - val_loss: 0.4648 - val_acc: 0.7854\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.4112 - acc: 0.8096 - val_loss: 0.4566 - val_acc: 0.8012\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.4036 - acc: 0.8137 - val_loss: 0.4436 - val_acc: 0.7968\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.3901 - acc: 0.8202 - val_loss: 0.4537 - val_acc: 0.7923\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.3731 - acc: 0.8313 - val_loss: 0.4548 - val_acc: 0.7997\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.3609 - acc: 0.8353 - val_loss: 0.4507 - val_acc: 0.8071\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.3457 - acc: 0.8462 - val_loss: 0.4724 - val_acc: 0.7958\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.3408 - acc: 0.8473 - val_loss: 0.4466 - val_acc: 0.8051\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.3315 - acc: 0.8515 - val_loss: 0.4438 - val_acc: 0.8091\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.3183 - acc: 0.8604 - val_loss: 0.4919 - val_acc: 0.7968\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.3035 - acc: 0.8679 - val_loss: 0.4911 - val_acc: 0.7963\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.2873 - acc: 0.8777 - val_loss: 0.5330 - val_acc: 0.7894\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.2712 - acc: 0.8868 - val_loss: 0.4899 - val_acc: 0.8115\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.2730 - acc: 0.8835 - val_loss: 0.4819 - val_acc: 0.8135\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.2410 - acc: 0.9000 - val_loss: 0.5623 - val_acc: 0.7958\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.2410 - acc: 0.9022 - val_loss: 0.6229 - val_acc: 0.7756\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.2234 - acc: 0.9085 - val_loss: 0.5301 - val_acc: 0.8022\n"
     ]
    }
   ],
   "source": [
    "history = classifier.fit_generator(training_set,\n",
    "                                   steps_per_epoch = 8000/32,\n",
    "                                   epochs = 25,\n",
    "                                   validation_data = test_set,\n",
    "                                   validation_steps = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method History.on_train_begin of <keras.callbacks.History object at 0x7f93b20ff0f0>>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='result'></a>\n",
    "### Result\n",
    "\n",
    "We can see a fairly good accuracy when using CNN for image classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
